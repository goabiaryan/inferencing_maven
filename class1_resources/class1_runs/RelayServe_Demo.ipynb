{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RelayServe Demo – Run and see it work\n",
        "\n",
        "This notebook starts a **mock backend** and **RelayServe**, then runs the API (non-streaming, streaming, request-id). No model download or llama.cpp required.\n",
        "\n",
        "**Run all cells in order.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: install RelayServe and add to path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RelayServe root: /path/to/class1_resources/RelayServe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Path to the RelayServe clone (this repo)\n",
        "RELAY_SERVE_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"RelayServe\"))\n",
        "if not os.path.isdir(RELAY_SERVE_ROOT):\n",
        "    RELAY_SERVE_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"class1_resources\", \"RelayServe\"))\n",
        "if not os.path.isdir(RELAY_SERVE_ROOT):\n",
        "    RELAY_SERVE_ROOT = os.path.abspath(os.getcwd())\n",
        "\n",
        "if RELAY_SERVE_ROOT not in sys.path:\n",
        "    sys.path.insert(0, RELAY_SERVE_ROOT)\n",
        "\n",
        "# Install RelayServe in editable mode if running from repo\n",
        "if os.path.isfile(os.path.join(RELAY_SERVE_ROOT, \"pyproject.toml\")):\n",
        "    get_ipython().system(f'pip install -e \"{RELAY_SERVE_ROOT}\" -q')\n",
        "else:\n",
        "    get_ipython().system('pip install relayserve -q')\n",
        "\n",
        "print(\"RelayServe root:\", RELAY_SERVE_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Start mock backend (fake LLM server)\n",
        "\n",
        "A minimal HTTP server that speaks OpenAI-style `/v1/chat/completions` so RelayServe has something to talk to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mock backend running at http://127.0.0.1:8091\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import threading\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "\n",
        "MOCK_BACKEND_PORT = 8091\n",
        "\n",
        "# Free port if still in use from a previous run (e.g. re-running this cell)\n",
        "subprocess.run(\n",
        "    f\"lsof -i :{MOCK_BACKEND_PORT} -t | xargs kill -9 2>/dev/null || true\",\n",
        "    shell=True, capture_output=True, timeout=2\n",
        ")\n",
        "import time\n",
        "time.sleep(0.3)\n",
        "\n",
        "class MockBackendHandler(BaseHTTPRequestHandler):\n",
        "    def do_POST(self):\n",
        "        if self.path.rstrip(\"/\").endswith(\"/v1/chat/completions\"):\n",
        "            length = int(self.headers.get(\"Content-Length\", 0))\n",
        "            body = self.rfile.read(length).decode(\"utf-8\") if length else \"{}\"\n",
        "            try:\n",
        "                data = json.loads(body)\n",
        "            except json.JSONDecodeError:\n",
        "                data = {}\n",
        "            stream = data.get(\"stream\", False)\n",
        "            messages = data.get(\"messages\", [])\n",
        "            prompt = \"\"\n",
        "            for m in messages:\n",
        "                if m.get(\"role\") == \"user\":\n",
        "                    prompt = str(m.get(\"content\", \"\"))\n",
        "                    break\n",
        "            reply = f\"Echo from mock backend: {prompt[:50]}...\" if len(prompt) > 50 else f\"Echo from mock backend: {prompt}\"\n",
        "\n",
        "            if stream:\n",
        "                self.send_response(200)\n",
        "                self.send_header(\"Content-Type\", \"text/event-stream\")\n",
        "                self.end_headers()\n",
        "                for word in reply.split():\n",
        "                    chunk = {\n",
        "                        \"id\": \"mock-1\",\n",
        "                        \"object\": \"chat.completion.chunk\",\n",
        "                        \"model\": \"mock\",\n",
        "                        \"choices\": [{\"index\": 0, \"delta\": {\"content\": word + \" \"}, \"finish_reason\": None}],\n",
        "                    }\n",
        "                    self.wfile.write(f\"data: {json.dumps(chunk)}\\n\\n\".encode(\"utf-8\"))\n",
        "                    self.wfile.flush()\n",
        "                self.wfile.write(b\"data: {\\\"choices\\\":[{\\\"delta\\\":{},\\\"finish_reason\\\":\\\"stop\\\"}]}\\n\\n\")\n",
        "                self.wfile.write(b\"data: [DONE]\\n\\n\")\n",
        "                self.wfile.flush()\n",
        "            else:\n",
        "                out = {\n",
        "                    \"id\": \"mock-1\",\n",
        "                    \"object\": \"chat.completion\",\n",
        "                    \"model\": \"mock\",\n",
        "                    \"choices\": [\n",
        "                        {\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": reply}, \"finish_reason\": \"stop\"}\n",
        "                    ],\n",
        "                    \"usage\": {\"prompt_tokens\": 1, \"completion_tokens\": 1, \"total_tokens\": 2},\n",
        "                }\n",
        "                self.send_response(200)\n",
        "                self.send_header(\"Content-Type\", \"application/json\")\n",
        "                body = json.dumps(out).encode(\"utf-8\")\n",
        "                self.send_header(\"Content-Length\", len(body))\n",
        "                self.end_headers()\n",
        "                self.wfile.write(body)\n",
        "        else:\n",
        "            self.send_response(404)\n",
        "            self.end_headers()\n",
        "\n",
        "    def log_message(self, *args):\n",
        "        pass\n",
        "\n",
        "mock_server = HTTPServer((\"127.0.0.1\", MOCK_BACKEND_PORT), MockBackendHandler)\n",
        "mock_thread = threading.Thread(target=mock_server.serve_forever, daemon=True)\n",
        "mock_thread.start()\n",
        "print(f\"Mock backend running at http://127.0.0.1:{MOCK_BACKEND_PORT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Start RelayServe\n",
        "\n",
        "RelayServe will proxy requests to the mock backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RelayServe running at http://127.0.0.1:8080\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"RELAYSERVE_BACKENDS\"] = f\"http://127.0.0.1:{MOCK_BACKEND_PORT}\"\n",
        "os.environ[\"RELAYSERVE_PORT\"] = \"8080\"\n",
        "\n",
        "from relayserve.internal.config.settings import Settings\n",
        "from relayserve.internal.server.app import build_app\n",
        "from relayserve.internal.server.http_server import run_server, _make_handler\n",
        "from http.server import ThreadingHTTPServer\n",
        "\n",
        "settings = Settings.from_env()\n",
        "app = build_app(settings)\n",
        "handler_factory = _make_handler(app)\n",
        "relay_server = ThreadingHTTPServer((\"127.0.0.1\", settings.port), handler_factory)\n",
        "relay_thread = threading.Thread(target=relay_server.serve_forever, daemon=True)\n",
        "relay_thread.start()\n",
        "\n",
        "import time\n",
        "time.sleep(0.5)\n",
        "print(f\"RelayServe running at http://127.0.0.1:{settings.port}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test: health and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GET /healthz -> 200\n",
            "{\n",
            "  \"status\": \"ok\"\n",
            "}\n",
            "\n",
            "GET /v1/models -> 200\n",
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": \"relay-gguf\",\n",
            "      \"object\": \"model\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import time\n",
        "\n",
        "def get(url):\n",
        "    with urllib.request.urlopen(url, timeout=5) as r:\n",
        "        return r.status, r.read().decode(\"utf-8\")\n",
        "\n",
        "base = \"http://127.0.0.1:8080\"\n",
        "# Wait for RelayServe to be ready (run cell 3 first)\n",
        "for attempt in range(15):\n",
        "    try:\n",
        "        get(base + \"/healthz\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if attempt == 0:\n",
        "            print(\"Waiting for RelayServe on 8080...\", end=\"\", flush=True)\n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        if attempt >= 14:\n",
        "            raise RuntimeError(\n",
        "                \"RelayServe not reachable at http://127.0.0.1:8080. \"\n",
        "                \"Run the previous cell (## 3. Start RelayServe) first.\"\n",
        "            ) from e\n",
        "        time.sleep(0.5)\n",
        "else:\n",
        "    print(\" OK\", flush=True)\n",
        "\n",
        "for path in [\"/healthz\", \"/v1/models\"]:\n",
        "    status, body = get(base + path)\n",
        "    print(f\"GET {path} -> {status}\")\n",
        "    print(json.dumps(json.loads(body), indent=2))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test: non-streaming chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status: 200\n",
            "X-Request-ID: 85c220ed42c440739bc958f1288a441b\n",
            "Reply: Echo from mock backend: Hello, RelayServe!\n",
            "id in body: 85c220ed42c440739bc958f1288a441b\n",
            "relay meta: {'device': 'cpu:arm (8 cores)', 'backend': 'llama.cpp', 'queue_ms': 13.586291985120624, 'ttft_ms': 15.841207990888506, 'batch_size': 1}\n"
          ]
        }
      ],
      "source": [
        "req = urllib.request.Request(\n",
        "    base + \"/v1/chat/completions\",\n",
        "    data=json.dumps({\n",
        "        \"model\": \"relay-gguf\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, RelayServe!\"}],\n",
        "    }).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"},\n",
        "    method=\"POST\",\n",
        ")\n",
        "with urllib.request.urlopen(req, timeout=10) as r:\n",
        "    print(\"Status:\", r.status)\n",
        "    print(\"X-Request-ID:\", r.headers.get(\"X-Request-ID\"))\n",
        "    body = r.read().decode(\"utf-8\")\n",
        "    data = json.loads(body)\n",
        "    print(\"Reply:\", data[\"choices\"][0][\"message\"][\"content\"])\n",
        "    print(\"id in body:\", data.get(\"id\"))\n",
        "    print(\"relay meta:\", data.get(\"relay\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test: request-id (client sends X-Request-ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X-Request-ID header: my-id-123\n",
            "id in body: my-id-123\n",
            "✓ Request-ID echoed correctly\n"
          ]
        }
      ],
      "source": [
        "req = urllib.request.Request(\n",
        "    base + \"/v1/chat/completions\",\n",
        "    data=json.dumps({\n",
        "        \"model\": \"relay-gguf\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "    }).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\", \"X-Request-ID\": \"my-id-123\"},\n",
        "    method=\"POST\",\n",
        ")\n",
        "with urllib.request.urlopen(req, timeout=10) as r:\n",
        "    print(\"X-Request-ID header:\", r.headers.get(\"X-Request-ID\"))\n",
        "    data = json.loads(r.read().decode(\"utf-8\"))\n",
        "    print(\"id in body:\", data.get(\"id\"))\n",
        "    assert data.get(\"id\") == \"my-id-123\", \"Request-ID should be echoed\"\n",
        "    print(\"✓ Request-ID echoed correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test: streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content-Type: text/event-stream\n",
            "X-Request-ID: stream-456\n",
            "Stream (first 1500 chars):\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"Echo \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"from \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"mock \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"backend: \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"Count \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"to \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"id\": \"stream-456\", \"object\": \"chat.completion.chunk\", \"model\": \"mock\", \"choices\": [{\"index\": 0, \"delta\": {\"content\": \"three \"}, \"finish_reason\": null}]}\n",
            "\n",
            "data: {\"choices\": [{\"delta\": {}, \"finish_reason\": \"stop\"}], \"id\": \"stream-456\", \"model\": \"relay-gguf\"}\n",
            "\n",
            "data: [DONE]\n",
            "\n",
            "...\n",
            "✓ Stream ends with data: [DONE]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "req = urllib.request.Request(\n",
        "    base + \"/v1/chat/completions\",\n",
        "    data=json.dumps({\n",
        "        \"model\": \"relay-gguf\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Count to three\"}],\n",
        "        \"stream\": True,\n",
        "    }).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\", \"X-Request-ID\": \"stream-456\"},\n",
        "    method=\"POST\",\n",
        ")\n",
        "with urllib.request.urlopen(req, timeout=60) as r:\n",
        "    print(\"Content-Type:\", r.headers.get(\"Content-Type\"))\n",
        "    print(\"X-Request-ID:\", r.headers.get(\"X-Request-ID\"))\n",
        "    print(\"Stream (first 1500 chars):\")\n",
        "    chunks = []\n",
        "    while True:\n",
        "        line = r.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        decoded = line.decode(\"utf-8\")\n",
        "        chunks.append(decoded)\n",
        "        if \"data: [DONE]\" in decoded:\n",
        "            break\n",
        "    body = \"\".join(chunks)\n",
        "    print(body[:1500])\n",
        "    if \"data: [DONE]\" in body:\n",
        "        print(\"...\\n✓ Stream ends with data: [DONE]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Done\n",
        "\n",
        "You’ve seen RelayServe:\n",
        "- **Health & models** – GET endpoints\n",
        "- **Non-streaming chat** – one JSON response with relay meta and usage\n",
        "- **Request-ID** – client sends `X-Request-ID`, server echoes it in header and body\n",
        "- **Streaming** – `stream: true` returns SSE until `data: [DONE]`\n",
        "\n",
        "**Next (Class 2):** For **config-backed routing**—one gateway, local + Modal backends selected by `model`—run [RelayServe_Class2_Demo.ipynb](../class2_runs/RelayServe_Class2_Demo.ipynb).\n",
        "\n",
        "To use a **real local model**, follow **Serve_local_model.md** (llama.cpp + GGUF)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
