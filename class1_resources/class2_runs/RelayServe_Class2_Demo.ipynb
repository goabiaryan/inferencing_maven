{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class 2 Demo – Config-backed routing (local + Modal)\n",
        "\n",
        "**Follow-up to Class 1:** In [RelayServe_Demo.ipynb](../class1_runs/RelayServe_Demo.ipynb) you ran one mock backend and RelayServe (env-based backends). Here we use **real backends**—local llama.cpp and Modal—with config-driven routing; the request’s `model` field.\n",
        "\n",
        "Here we use **real backends**: your **local llama.cpp** (port 8081) and a **Modal** deployment, with config-driven routing so one gateway can talk to both.\n",
        "\n",
        "**Prerequisites (before running):**\n",
        "1. **Local llama.cpp** running on port 8081 (e.g. `scripts/spawn_backends.py` — see [Serve_local_model.md](../class1_runs/Serve_local_model.md)).\n",
        "2. **Modal** deployed and web URL ready: `cd modal && modal deploy modal_llama_server.py`, then get the `https://...modal.run` URL from the output or run `modal run modal_llama_server.py` once to see it. Paste that URL in section 2.\n",
        "\n",
        "**Run all cells in order.** Section 1b frees port 8080 so RelayServe can bind; then we write config with your URLs, start RelayServe, and send `model=local` and `model=modal` requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: path and install RelayServe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1b. Free port 8080 (so RelayServe can bind)\n",
        "\n",
        "Run this once at the start so nothing else is on 8080. We do not touch 8081 (your local llama). If you re-run the notebook without restarting the kernel, run section 9 at the end first, then run from here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Port 8080 freed. You can start RelayServe.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "subprocess.run(\n",
        "    \"lsof -i :8080 -t | xargs kill -9 2>/dev/null || true\",\n",
        "    shell=True, capture_output=True, timeout=2\n",
        ")\n",
        "print(\"Port 8080 freed. You can start RelayServe.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RelayServe root: /path/to/class1_resources/RelayServe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "RELAY_SERVE_ROOT = os.path.join(REPO_ROOT, \"RelayServe\")\n",
        "if not os.path.isdir(RELAY_SERVE_ROOT):\n",
        "    RELAY_SERVE_ROOT = os.path.abspath(os.getcwd())\n",
        "\n",
        "if RELAY_SERVE_ROOT not in sys.path:\n",
        "    sys.path.insert(0, RELAY_SERVE_ROOT)\n",
        "\n",
        "if os.path.isfile(os.path.join(RELAY_SERVE_ROOT, \"pyproject.toml\")):\n",
        "    get_ipython().system(f'pip install -e \"{RELAY_SERVE_ROOT}\" -q')\n",
        "else:\n",
        "    get_ipython().system('pip install relayserve -q')\n",
        "\n",
        "print(\"RelayServe root (path hidden)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Set backend URLs and write config\n",
        "\n",
        "Set your **local llama.cpp** URL (default port 8081) and your **Modal** web URL. If you haven't deployed Modal yet: `cd modal && modal deploy modal_llama_server.py`, then run `modal run modal_llama_server.py` once to see the `https://...modal.run` URL. We backup the existing config and write one that points to these backends; it's restored at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config written: local=http://127.0.0.1:8081, modal=https://<your-modal-app>.modal.run\n"
          ]
        }
      ],
      "source": [
        "LOCAL_LLAMA_URL = \"http://127.0.0.1:8081\"\n",
        "MODAL_URL = \"https://<your-modal-app>.modal.run\"\n",
        "\n",
        "config_path = os.path.join(RELAY_SERVE_ROOT, \"config.yaml\")\n",
        "config_backup = None\n",
        "if os.path.isfile(config_path):\n",
        "    with open(config_path) as f:\n",
        "        config_backup = f.read()\n",
        "\n",
        "real_config = f\"\"\"\n",
        "default_backend: local\n",
        "\n",
        "backends:\n",
        "  local:\n",
        "    type: local\n",
        "    url: {LOCAL_LLAMA_URL}\n",
        "  modal:\n",
        "    type: modal\n",
        "    url: {MODAL_URL.rstrip(\"/\")}\n",
        "\"\"\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    f.write(real_config)\n",
        "print(f\"Config written: local={LOCAL_LLAMA_URL}, modal={MODAL_URL}\")\n",
        "if \"YOUR_WORKSPACE\" in MODAL_URL or not MODAL_URL.startswith(\"https://\"):\n",
        "    print(\"  → Update MODAL_URL above with your Modal web URL, then re-run this cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ensure backends are running\n",
        "\n",
        "Your **local llama.cpp** (port 8081) and **Modal** must already be running. If not, start local llama with `scripts/spawn_backends.py` and deploy Modal with `scripts/deploy_modal.sh`. The cell below optionally checks that both URLs are reachable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local (http://127.0.0.1:8081): not reachable (is llama running on 8081?)\n",
            "Modal (https://<your-modal-app>.modal.r...): not reachable or URL not set\n",
            "  → Start local llama: scripts/spawn_backends.py (see Serve_local_model.md)\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "def check(url, timeout=3):\n",
        "    try:\n",
        "        req = urllib.request.Request(url, method=\"GET\")\n",
        "        urllib.request.urlopen(req, timeout=timeout)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "local_ok = check((LOCAL_LLAMA_URL or \"\").rstrip(\"/\") + \"/health\") if LOCAL_LLAMA_URL else False\n",
        "modal_ok = check((MODAL_URL or \"\").rstrip(\"/\") + \"/health\") if (MODAL_URL and MODAL_URL.startswith(\"https://\") and \"YOUR_WORKSPACE\" not in MODAL_URL) else False\n",
        "print(f\"Local ({LOCAL_LLAMA_URL}): {'OK' if local_ok else 'not reachable (is llama running on 8081?)'}\")\n",
        "print(f\"Modal ({MODAL_URL[:50]}...): {'OK' if modal_ok else 'not reachable or URL not set'}\")\n",
        "if not local_ok:\n",
        "    print(\"  → Start local llama: scripts/spawn_backends.py (see Serve_local_model.md)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. (Real Modal — no mock)\n",
        "\n",
        "Using your deployed Modal backend. No mock server; ensure Modal is deployed and MODAL_URL is set in section 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real Modal backend; no mock. Ensure Modal is deployed and MODAL_URL set in section 2.\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Start RelayServe (with config-backed router)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RelayServe running at http://127.0.0.1:8080 (config: real local + modal)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import threading\n",
        "\n",
        "REPO_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "RELAY_SERVE_ROOT = os.path.join(REPO_ROOT, \"RelayServe\")\n",
        "if not os.path.isdir(RELAY_SERVE_ROOT):\n",
        "    RELAY_SERVE_ROOT = os.path.abspath(os.getcwd())\n",
        "if RELAY_SERVE_ROOT not in sys.path:\n",
        "    sys.path.insert(0, RELAY_SERVE_ROOT)\n",
        "\n",
        "os.environ[\"RELAYSERVE_ROOT\"] = RELAY_SERVE_ROOT\n",
        "os.environ[\"RELAYSERVE_PORT\"] = \"8080\"\n",
        "os.environ[\"RELAYSERVE_BACKENDS\"] = \"\"\n",
        "\n",
        "from relayserve.internal.config.settings import Settings\n",
        "from relayserve.internal.server.app import build_app\n",
        "from relayserve.internal.server.http_server import run_server, _make_handler\n",
        "from http.server import ThreadingHTTPServer\n",
        "\n",
        "settings = Settings.from_env()\n",
        "app = build_app(settings)\n",
        "handler_factory = _make_handler(app)\n",
        "relay_server = ThreadingHTTPServer((\"127.0.0.1\", settings.port), handler_factory)\n",
        "threading.Thread(target=relay_server.serve_forever, daemon=True).start()\n",
        "\n",
        "print(\"RelayServe running at http://127.0.0.1:8080 (config: real local + modal)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Request with model=local\n",
        "\n",
        "Run this **after** sections 3 (ensure backends) and 5 (Start RelayServe). If you see \"RelayServe not ready\", run the Start RelayServe cell again, then retry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model=local: Echo: Say hello\n",
            "relay backend: metal\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import urllib.request\n",
        "\n",
        "def _parse_response(body):\n",
        "    try:\n",
        "        return json.loads(body)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    plain = re.sub(r\"\\033\\[[0-9;]*m\", \"\", body)\n",
        "    reply = \"\"\n",
        "    backend = \"\"\n",
        "    for line in plain.split(\"\\n\"):\n",
        "        if line.startswith(\"Reply:\"):\n",
        "            reply = line.split(\"Reply:\", 1)[1].strip()\n",
        "        elif line.startswith(\"Backend:\"):\n",
        "            backend = line.split(\"Backend:\", 1)[1].strip()\n",
        "    return {\"choices\": [{\"message\": {\"content\": reply}}], \"relay\": {\"backend\": backend}}\n",
        "\n",
        "last_err = None\n",
        "out = None\n",
        "for attempt in range(8):\n",
        "    try:\n",
        "        req = urllib.request.Request(\n",
        "            \"http://127.0.0.1:8080/v1/chat/completions\",\n",
        "            data=json.dumps({\"model\": \"local\", \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}], \"stream\": False}).encode(\"utf-8\"),\n",
        "            headers={\"Content-Type\": \"application/json\"},\n",
        "            method=\"POST\",\n",
        "        )\n",
        "        with urllib.request.urlopen(req, timeout=5) as r:\n",
        "            body = r.read().decode(\"utf-8\")\n",
        "            if not body.strip():\n",
        "                status = getattr(r, \"status\", \"?\")\n",
        "                last_err = RuntimeError(f\"Empty response (HTTP {status})\")\n",
        "                if attempt == 0:\n",
        "                    print(f\"RelayServe returned empty body (HTTP {status}). Retrying...\")\n",
        "                continue\n",
        "            out = _parse_response(body)\n",
        "            break\n",
        "    except Exception as e:\n",
        "        last_err = e\n",
        "else:\n",
        "    msg = \"RelayServe not ready. Run section 1b (Free ports), then sections 3–5 (mocks + RelayServe), then retry.\"\n",
        "    if last_err:\n",
        "        msg += f\" Last error: {last_err}\"\n",
        "    raise RuntimeError(msg)\n",
        "\n",
        "reply = out.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "relay_meta = out.get(\"relay\", {})\n",
        "print(\"model=local:\", reply)\n",
        "print(\"relay backend:\", relay_meta.get(\"backend\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Request with model=modal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model=modal: Echo: Explain KV cache\n",
            "relay backend: metal\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import urllib.request\n",
        "\n",
        "req = urllib.request.Request(\n",
        "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
        "    data=json.dumps({\"model\": \"modal\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain KV cache\"}], \"stream\": False}).encode(\"utf-8\"),\n",
        "    headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"},\n",
        "    method=\"POST\",\n",
        ")\n",
        "with urllib.request.urlopen(req, timeout=10) as r:\n",
        "    body = r.read().decode(\"utf-8\")\n",
        "if not body.strip():\n",
        "    raise RuntimeError(f\"RelayServe returned empty body (HTTP {getattr(r, 'status', '?')}). Run sections 1b and 3–5 first, then retry.\")\n",
        "try:\n",
        "    out = json.loads(body)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Body (first 300 chars):\", body[:300] if body else \"(empty)\")\n",
        "    raise RuntimeError(f\"Invalid JSON from RelayServe: {e}\") from e\n",
        "\n",
        "reply = out.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
        "relay_meta = out.get(\"relay\", {})\n",
        "print(\"model=modal:\", reply)\n",
        "print(\"relay backend:\", relay_meta.get(\"backend\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Restore config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config.yaml restored.\n"
          ]
        }
      ],
      "source": [
        "if config_backup is not None:\n",
        "    with open(config_path, \"w\") as f:\n",
        "        f.write(config_backup)\n",
        "    print(\"config.yaml restored.\")\n",
        "else:\n",
        "    print(\"No backup to restore.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Free port 8080\n",
        "\n",
        "Run this at the end to free port 8080 (RelayServe). We do not kill the current process; we only kill other PIDs so the kernel is not stopped. Port 8081 (local llama) is left as-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Port 8080 freed (current process skipped).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "my_pid = os.getpid()\n",
        "r = subprocess.run(\n",
        "    \"lsof -i :8080 -t\",\n",
        "    shell=True, capture_output=True, timeout=2, text=True\n",
        ")\n",
        "pids = (r.stdout or \"\").strip().split()\n",
        "for pid in pids:\n",
        "    try:\n",
        "        if int(pid) != my_pid:\n",
        "            subprocess.run(f\"kill -9 {pid}\", shell=True, capture_output=True, timeout=1)\n",
        "    except (ValueError, OSError):\n",
        "        pass\n",
        "print(\"Port 8080 freed (current process skipped).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
